{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "348cdb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import numpy as np                # Numpy for array manipulation for ease of access\n",
    "import torch                      # Pytorch for array manipulation on the GPU and nice deep learning functions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision import transforms as tfs\n",
    "import cv2\n",
    "# Image import and display libraries                       # OpenCV for image processing\n",
    "import matplotlib.pyplot as plt   # Plotting functions\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# Image processing libraries for image feature extractor\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.ndimage import generic_filter\n",
    "from skimage.filters import laplace, gabor\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# A few more tools\n",
    "from sklearn import svm           # SVM classifier library\n",
    "import os                         # Navigate through directories\n",
    "import csv                        # Read in a CSV file\n",
    "import time                       # Timing function\n",
    "import pickle                     # Saving and loading variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c80a5125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "use_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    use_gpu = True\n",
    "    print(\"using cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b323d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeratomaDataset(data.Dataset):\n",
    "    def __init__(self, root_folder, image_transforms=None, gt_transforms=None):\n",
    "        '''\n",
    "            reference dataset loading for TeratomaDataset\n",
    "            root_folder: the root_folder of the TeratomaDataset \n",
    "            set_indices: is the indices for sets to be used\n",
    "            subset_indices: is the indices for the subsets to be used\n",
    "            split: 'train', 'val' or 'test'\n",
    "            domain: the image domains to be loaded.\n",
    "            image_transforms: any transforms to perform, can add augmentations here.\n",
    "            gt_transforms: list of bool. Indicates whether image_transforms should also be appleid to gt.\n",
    "        '''\n",
    "\n",
    "        self.input_folder = osp.join(root_folder, 'input')\n",
    "        self.seg_folders = {\n",
    "            'GI': osp.join(root_folder, 'segmentation', 'GI'),\n",
    "            'Cartilage': osp.join(root_folder, 'segmentation', 'Cartilage'),\n",
    "            'RPE': osp.join(root_folder, 'segmentation', 'RPE'),\n",
    "            'Neural': osp.join(root_folder, 'segmentation', 'Neural'),\n",
    "        }\n",
    "\n",
    "        self.image_paths = sorted([\n",
    "            osp.join(self.input_folder, fname)\n",
    "            for fname in os.listdir(self.input_folder)\n",
    "            if fname.endswith('.png')\n",
    "        ])\n",
    "\n",
    "        self.image_transforms = image_transforms\n",
    "        self.gt_transforms = gt_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image_name = osp.basename(image_path)\n",
    "\n",
    "        # Load input image, normalize\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "\n",
    "        # Load 4 segmentation layers\n",
    "        gt_layers = []\n",
    "        for key in ['GI', 'Cartilage', 'RPE', 'Neural']:\n",
    "            seg_path = osp.join(self.seg_folders[key], image_name)\n",
    "            seg = Image.open(seg_path).convert('L')\n",
    "            seg = (np.array(seg) > 0).astype(np.uint8)  # binary mask\n",
    "            gt_layers.append(seg)\n",
    "\n",
    "        gt = np.stack(gt_layers, axis=0)  # shape: (4, H, W)\n",
    "        target = torch.from_numpy(gt).long()  # shape (H, W)\n",
    "\n",
    "        # Transforms\n",
    "        if self.image_transforms:\n",
    "            image = self.image_transforms(image)\n",
    "        else:\n",
    "            image = tfs.ToTensor()(image)\n",
    "\n",
    "        if self.gt_transforms:\n",
    "            gt = self.gt_transforms(gt)\n",
    "        else:\n",
    "            gt = torch.from_numpy(gt).float()  # (4, H, W)\n",
    "\n",
    "        print(image.shape)\n",
    "        print(target.shape)\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dfde38",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "163db814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_conv_stage(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=True, useBN=False):\n",
    "  if useBN:\n",
    "    return nn.Sequential(\n",
    "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.BatchNorm2d(dim_out),\n",
    "      nn.LeakyReLU(0.1),\n",
    "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.BatchNorm2d(dim_out),\n",
    "      nn.LeakyReLU(0.1)\n",
    "    )\n",
    "  else:\n",
    "    return nn.Sequential(\n",
    "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "def upsample(ch_coarse, ch_fine):\n",
    "  return nn.Sequential(\n",
    "    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False),\n",
    "    nn.ReLU()\n",
    "  )\n",
    "\n",
    "class unet(nn.Module):\n",
    "  def __init__(self, useBN=False):    #Change useBN to True\n",
    "    super(unet, self).__init__()\n",
    "    # Downgrade stages\n",
    "    self.conv1   = add_conv_stage(3, 32, useBN=useBN)\n",
    "    self.conv2   = add_conv_stage(32, 64, useBN=useBN)\n",
    "    self.conv3   = add_conv_stage(64, 128, useBN=useBN)\n",
    "    self.conv4   = add_conv_stage(128, 256, useBN=useBN)\n",
    "    self.conv5   = add_conv_stage(256, 512, useBN=useBN)\n",
    "    # Upgrade stages\n",
    "    self.conv4m = add_conv_stage(512, 256, useBN=useBN)\n",
    "    self.conv3m = add_conv_stage(256, 128, useBN=useBN)\n",
    "    self.conv2m = add_conv_stage(128,  64, useBN=useBN)\n",
    "    self.conv1m = add_conv_stage( 64,  32, useBN=useBN)\n",
    "    # Maxpool\n",
    "    self.max_pool = nn.MaxPool2d(2)\n",
    "    # Upsample layers\n",
    "    self.upsample54 = upsample(512, 256)\n",
    "    self.upsample43 = upsample(256, 128)\n",
    "    self.upsample32 = upsample(128,  64)\n",
    "    self.upsample21 = upsample(64 ,  32)\n",
    "\n",
    "    ## TODO last layer & activations\n",
    "    self.output_conv = nn.Conv2d(32, 4, 1)\n",
    "    # self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    #TODO implement forward function\n",
    "    # Downgrade\n",
    "    x1 = self.conv1(x)                    #1 -> 32 channels\n",
    "    x2 = self.conv2(self.max_pool(x1))    #32 -> 64 channels\n",
    "    x3 = self.conv3(self.max_pool(x2))    #64 -> 128 channels\n",
    "    x4 = self.conv4(self.max_pool(x3))    #128 -> 256 channels\n",
    "    x5 = self.conv5(self.max_pool(x4))    #256 -> 512 channels\n",
    "\n",
    "    # Upgrade\n",
    "    x4m = self.upsample54(x5)             #512 -> 256 channels\n",
    "    x4m = torch.cat((x4, x4m), dim=1)\n",
    "    x4m = self.conv4m(x4m)\n",
    "\n",
    "    x3m = self.upsample43(x4m)            #256 -> 128 channels\n",
    "    x3m = torch.cat((x3, x3m), dim=1)\n",
    "    x3m = self.conv3m(x3m)\n",
    "\n",
    "    x2m = self.upsample32(x3m)            #128 -> 64 channels\n",
    "    x2m = torch.cat((x2, x2m), dim=1)\n",
    "    x2m = self.conv2m(x2m)\n",
    "\n",
    "    x1m = self.upsample21(x2m)            #64 -> 32 channels\n",
    "    x1m = torch.cat((x1, x1m), dim=1)\n",
    "    x1m = self.conv1m(x1m)\n",
    "\n",
    "    # Output\n",
    "    output = self.output_conv(x1m)      #32 -> 4 channels\n",
    "    # output = self.activation(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0418158",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7136cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainning(model, trainning_dataloader, validation_dataloader, num_epochs, criterion, optimizer, filename):\n",
    "    if use_gpu:\n",
    "      model.cuda()\n",
    "    lr_changed = False\n",
    "    trainning_losses = []\n",
    "    validation_losses = []\n",
    "    total_training_loss = 0\n",
    "    total_val_loss = 0\n",
    "    total_training_iteration = 0\n",
    "    total_val_iteration = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        i = 0\n",
    "        model.train()\n",
    "        for data in trainning_dataloader:\n",
    "          img,y = data\n",
    "          if use_gpu:\n",
    "            img = img.cuda()\n",
    "            y = y.cuda()\n",
    "          out = model(img)\n",
    "          optimizer.zero_grad()\n",
    "          loss = criterion(out, y)\n",
    "          total_training_loss += loss.item()\n",
    "          # loss.backward()\n",
    "          optimizer.step()\n",
    "          i = i+1\n",
    "          total_training_iteration += 1\n",
    "          if total_training_iteration % 100 == 99:\n",
    "            trainning_losses.append(total_training_loss / total_training_iteration)\n",
    "        if epoch % 5 == 4:\n",
    "            print(\"learning_rate decayed\")\n",
    "            for param_group in optimizer.param_groups:\n",
    "              param_group['lr'] *= 0.1\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "          for data in validation_dataloader:\n",
    "            img,y = data\n",
    "            if use_gpu:\n",
    "              img = img.cuda()\n",
    "              y = y.cuda()\n",
    "            out = model(img)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(out, y)\n",
    "            total_val_loss += loss.item()\n",
    "            total_val_iteration += 1\n",
    "            if total_val_iteration % 100 == 99:\n",
    "              validation_losses.append(total_val_loss / total_val_iteration)\n",
    "        print(\"epoch:\",epoch,\"training_loss:\",total_training_loss / total_training_iteration, \"validation_loss:\",total_val_loss / total_val_iteration)\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        \n",
    "    # Plotting training and validation losses in subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    ax1.plot(trainning_losses, label='Training Loss')\n",
    "    ax1.set_title(\"Training Loss\")\n",
    "    ax1.set_xlabel(\"Iterations (x100)\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    \n",
    "    # Plot validation loss\n",
    "    ax2.plot(validation_losses, label='Validation Loss')\n",
    "    ax2.set_title(\"Validation Loss\")\n",
    "    ax2.set_xlabel(\"Iterations (x100)\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea6ac1",
   "metadata": {},
   "source": [
    "### Define transforms and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "add17989",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = r\"C:\\Users\\willllllli\\Documents\\Dr. Z lab\\Teratoma_Segmentation\\DATA\\20240606165451_Dec 2023 E32C6 L4i undiff\"\n",
    "\n",
    "mean = [0.68358143, 0.5290253,  0.61094054] \n",
    "std = [0.11946411, 0.2183087,  0.09772461]\n",
    "size = (512, 512)\n",
    "\n",
    "image_transforms = tfs.Compose([\n",
    "    tfs.Resize(size, interpolation=tfs.InterpolationMode.BILINEAR),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "gt_transforms = tfs.Compose([\n",
    "    tfs.Resize(size, interpolation=tfs.InterpolationMode.NEAREST),\n",
    "    tfs.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94cebbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = TeratomaDataset(\n",
    "    root_folder=root_folder,\n",
    "    image_transforms=image_transforms,\n",
    "    gt_transforms=gt_transforms\n",
    ")\n",
    "\n",
    "N = len(full_ds)\n",
    "train_N = int(0.8 * N)\n",
    "val_N   = int(0.1 * N)\n",
    "test_N  = N - train_N - val_N  # ensure sums to N\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_ds, val_ds, test_ds = random_split(full_ds, [train_N, val_N, test_N], generator=generator)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac1c66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_demo(model, test_dataset_loader, num=10):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    for demo in test_dataset_loader:\n",
    "        demo_input, demo_target = demo\n",
    "        if use_gpu:\n",
    "           demo_input = demo_input.cuda()\n",
    "        demo_output = model(demo_input)\n",
    "        for i in range(demo_input.shape[0]):\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(demo_input[i].permute(1,2,0).detach().cpu().numpy())\n",
    "            plt.axis(\"off\")\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(demo_output[i].detach().cpu().numpy().squeeze()*255)\n",
    "            plt.axis(\"off\")\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(demo_target[i].detach().numpy().squeeze())\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        if count >= num:\n",
    "          break\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd2c1f0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m criterion = nn.BCEWithLogitsLoss()\n\u001b[32m      3\u001b[39m segmentation_optimizer = torch.optim.Adam(segmentation_model.parameters(), lr=LEARNING_RATE, weight_decay=\u001b[32m1e-4\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainning\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmentation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfinal_vanilla_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrainning\u001b[39m\u001b[34m(model, trainning_dataloader, validation_dataloader, num_epochs, criterion, optimizer, filename)\u001b[39m\n\u001b[32m     12\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m model.train()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrainning_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m  \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mTeratomaDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Transforms\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.image_transforms:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     57\u001b[39m     image = tfs.ToTensor()(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torchvision\\transforms\\functional.py:465\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) != \u001b[32m1\u001b[39m:\n\u001b[32m    460\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    461\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m _, image_height, image_width = \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    467\u001b[39m     size = [size]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torchvision\\transforms\\functional.py:80\u001b[39m, in \u001b[36mget_dimensions\u001b[39m\u001b[34m(img)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t.get_dimensions(img)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willllllli\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[39m, in \u001b[36mget_dimensions\u001b[39m\u001b[34m(img)\u001b[39m\n\u001b[32m     29\u001b[39m     width, height = img.size\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "segmentation_model = unet(useBN=True)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "segmentation_optimizer = torch.optim.Adam(segmentation_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "trainning(segmentation_model, train_loader, val_loader,  NUM_EPOCHS, criterion, segmentation_optimizer, \"final_vanilla_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
